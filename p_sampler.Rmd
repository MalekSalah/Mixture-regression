---
title: "Sampling algorithm"
output:
  html_document:
---

```{r}
library(ggplot2)
library(aTSA)
```

#### [The model:]{.underline}

We suppose we have an n-sample of the random variable $Y$ and want to estimate the mixture parameter using a Bayesian approach.

$$
Y|\mu_1,\sigma_1,\mu_2,\sigma_2,p\sim p \mathcal{N}(\mu_1,\sigma_1)+(1-p) \mathcal{N}(\mu_2,\sigma_2).
$$

```{r}
p <- 0.7 #Mixture weight

# Component parameters
sigma_1 <- 1 
sigma_2 <- 1
mu_1 <- 4 
mu_2 <- 0

Theta1 = list("mu"=mu_1, "sigma"= sigma_1)
Theta2 = list("mu"=mu_2, "sigma"= sigma_2)
```

we use a mixture sampling algorithm to create our sample

```{r echo=TRUE}
# Number of samples from the mixture distribution
N <- 3000
# Sample N uniforms U
B <- sample(1:2, size = N, replace = TRUE, prob = c(p, 1-p))

# Sampling from the mixture
Mu <- c(mu_1, mu_2)
Std <- c(sigma_1, sigma_2)
Y <- rnorm(N, mean = Mu[B], sd = Std[B])

# Plotting the distributions
ggplot() + 
  geom_histogram(aes(Y), bins = 100)+
  ggtitle("Historgram of the mixture distribution Y")
```

#### [Sampling the mixture weight p :]{.underline}

In this section we suppose that there is only one parameter of interest and suppose all the others constants equal to some arbitrary values.

##### Prior distribution:

The parameter $p$ lies on $[0,1]$ which suggests a Beta like distribution for the prior

$$
p \sim  \mathcal{B}(\tau_0, \tau_1)
$$

```{r}
Tau <- list("tau1"=0.5 , "tau2"=0.5)
```

##### The likelihood:

In the case of our model, the likelihood function is given by

$$
\ell(\Theta|y,X) = \prod_{i=1}^{n} \frac{p}{\sqrt{2\pi}\sigma_1} \exp \left( -\frac{(y_i-\mu_1)^2}{2\sigma_1^2} \right) + \frac{1-p}{\sqrt{2\pi}\sigma_2} \exp \left( -\frac{(y_i-\mu_2)^2}{2\sigma_2^2} \right).
$$

```{r}
Log_lik <- function (p, Theta1, Theta2, Y){
  return(sum(log(p * dnorm(Y, Theta1$mu, Theta1$sigma) + (1-p) * dnorm(Y, Theta2$mu, Theta2$sigma))))
}
```

##### Posterior distribution:

The posterior distribution we want to sample from is

$$
\pi(p \mid \Theta,y,X) = \frac{\ell(\Theta|y,X)}{B(\tau_0, \tau_1)} p^{\tau_0-1}(1-p)^{\tau_1-1}, \qquad B(\tau_0, \tau_1) = \frac{\Gamma(\tau_0)\Gamma(\tau_1)}{\Gamma(\tau_0+\tau_1)} 
$$

```{r}
Log_post_p <- function (p, Theta1, Theta2, Tau, Y) {
  return(Log_lik(p, Theta1, Theta2, Y) + log(dbeta(p, Tau$tau1, Tau$tau2)))
}
```

##### Metropolis-Hastings sampler:

```{r}
# Number of iterations
T <- 50000

##------------- Sampling algorithm
# Initialization
eps_p <- 0.6
p_0 <- 0.9
P <- p_0
Acceptance <- NULL


for (i in 1:T){    
  # Generating a proposal
  p_t <- P[length(P)]
  p_prop <- rbeta(1, p_t*eps_p+1, (1-p_t)*eps_p+1)
  
  # Computing acceptance probability
  log_acceptance <- (Log_post_p(p_prop, Theta1, Theta2, Tau, Y)+log(dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1))) - (Log_post_p(p_t, Theta1, Theta2, Tau, Y) + log(dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1)))
  acc <- min(1, exp(log_acceptance))
  
  # Decision on the next step 
  U = runif(1)
  if ( acc > U) P <- c(P, p_prop) 
  else P <- c(P, p_t)
}
```

#### [Results:]{.underline}

```{r}
ggplot() +
  geom_line(aes(y=P, x=seq(1,length(P))))
```

The Markov chain constructed by the Metropolis-Hastings algorithm quickly starts to fluctuate around the exact value of the parameter. Since we're more interested in the stationary part of the chain, we can discard the first few elements and focus on the fluctuation around the exact value of $p$.

```{r}
n_trunc <- 100 # Index at which we start the truncated Markov Chain
P <- P[n_trunc:length(P)]
```

The histogram bellow shows the distribution of the obtained sample which is yet to be confirmed as Gaussian (According to the Bernstein-Von Mises theorem). One important observation is that the Bayesian estimator (in blue) is close to the exact value of the parameter (in red).

```{r}
ggplot() +
  geom_histogram(aes(x=P), bins = 20) +
  geom_vline(xintercept = p, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(P), color="blue", show.legend = TRUE)

```

We now try to asses the stationary of our Markov chain by studying the autocorrelation and performing both ACF and PACF techniques as well as

```{r}
ggplot() +
  geom_line(aes(y=P, x=seq(1,length(P))))
```

```{r}
acf(P_no_outliers, lag.max = 100)
pacf(P_no_outliers, lag.max = 100)
```

```{r}

```
