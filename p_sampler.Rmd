---
---
---

```{r}
library(ggplot2)
library(reshape2)
library(tseries)
library(gridExtra)
library(grid)
library(dplyr)
library(compositions)
```

# [The model:]{.underline}

We suppose we have an n-sample of the random variable $Y$ and want to estimate the mixture parameter using a Bayesian approach.

$$
Y|\mu_1,\sigma_1,\mu_2,\sigma_2,p\sim p \mathcal{N}(\mu_1,\sigma_1)+(1-p) \mathcal{N}(\mu_2,\sigma_2).
$$

```{r}
p <- 0.2 #Mixture weight

# Component parameters
sigma_1 <- 1
sigma_2 <- 2
mu_1 <- 2 
mu_2 <- 10

Theta1 = list("mu"=mu_1, "sigma"= sigma_1)
Theta2 = list("mu"=mu_2, "sigma"= sigma_2)
```

we use a mixture sampling algorithm to create our sample

```{r echo=TRUE}
# Number of samples from the mixture distribution
N <- 3000
# Sample N uniforms U
B <- sample(1:2, size = N, replace = TRUE, prob = c(p, 1-p))

# Sampling from the mixture
Mu <- c(mu_1, mu_2)
Std <- c(sigma_1, sigma_2)
Y <- rnorm(N, mean = Mu[B], sd = Std[B])
sigma <- sd(Y) 
mu <- mean(Y)
# Plotting the distributions
ggplot() + 
  geom_histogram(aes(Y), bins = 100)+
  ggtitle("Historgram of the mixture distribution Y")
```

## [Sampling the mixture weight p :]{.underline}

In this section we suppose that there is only one parameter of interest and suppose all the others constants equal to some arbitrary values.

##### Prior distribution:

The parameter $p$ lies on $[0,1]$ which suggests a Beta like distribution for the prior

$$
p \sim  \mathcal{B}(\tau_0, \tau_1)
$$

```{r}
Tau <- list("tau1"=0.5 , "tau2"=0.5)
```

##### The likelihood:

In the case of our model, the likelihood function is given by

$$
\ell(\Theta|y,X) = \prod_{i=1}^{n} \frac{p}{\sqrt{2\pi}\sigma_1} \exp \left( -\frac{(y_i-\mu_1)^2}{2\sigma_1^2} \right) + \frac{1-p}{\sqrt{2\pi}\sigma_2} \exp \left( -\frac{(y_i-\mu_2)^2}{2\sigma_2^2} \right).
$$

```{r}
Log_lik <- function (p, Theta1, Theta2, Y){
  res <- vector(mode = "numeric", length(p))
  for (i in 1:length(p)){
    res[i] <- sum(log(p[i] * dnorm(Y, Theta1$mu, Theta1$sigma) + (1-p[i]) * dnorm(Y, Theta2$mu, Theta2$sigma)))
  }
  return(res)
}
```

##### Posterior distribution:

The posterior distribution we want to sample from is

$$
\pi(p \mid \Theta,y,X) = \frac{\ell(\Theta|y,X)}{B(\tau_0, \tau_1)} p^{\tau_0-1}(1-p)^{\tau_1-1}, \qquad B(\tau_0, \tau_1) = \frac{\Gamma(\tau_0)\Gamma(\tau_1)}{\Gamma(\tau_0+\tau_1)} 
$$

```{r}
Log_post_p <- function (p, Theta1, Theta2, Tau, Y) {
  return(Log_lik(p, Theta1, Theta2, Y) + log(dbeta(p, Tau$tau1, Tau$tau2)))
}
```

##### Metropolis-Hastings sampler:

```{r}
# Number of iterations
T <- 50000

##------------- Sampling algorithm
# Initialization
eps_p <- 0.4
p_0 <- 0.9
P <- p_0
Acceptance <- NULL


for (i in 1:T){    
  # Generating a proposal
  p_t <- P[length(P)]
  p_prop <- rbeta(1, p_t*eps_p+1, (1-p_t)*eps_p+1)
  
  # Computing acceptance probability
  log_acceptance <- (Log_post_p(p_prop, Theta1, Theta2, Tau, Y)+log(dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1))) - (Log_post_p(p_t, Theta1, Theta2, Tau, Y) + log(dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1)))
  acc <- min(1, exp(log_acceptance))
  
  # Decision on the next step 
  U = runif(1)
  if ( acc > U) P <- c(P, p_prop) 
  else P <- c(P, p_t)
}
```

### [Results:]{.underline}

```{r}
ggplot() +
  geom_line(aes(y=P, x=seq(1,length(P))))
```

The Markov chain constructed by the Metropolis-Hastings algorithm quickly starts to fluctuate around the exact value of the parameter. Since we're more interested in the stationary part of the chain, we can discard the first few elements and focus on the fluctuation around the exact value of $p$.

```{r}
n_trunc <- 100 # Index at which we start the truncated Markov Chain
P <- P[n_trunc:length(P)]
```

The histogram bellow shows the distribution of the obtained sample. One important observation is that the Bayesian estimator (in blue) is close to the exact value of the parameter (in red).

```{r warning=FALSE}
ggplot() +
  geom_histogram(aes(x=P), bins = 1000) +
  geom_vline(xintercept = p, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(P), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de P\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")+
  xlim(0,1)

```

The obtained distribution is a beta distribution with very high parameters which explains the peaked value around the mean.

#### Stationary of the Markov chain

We now try to asses the stationary of our Markov chain by studying the autocorrelation and performing both ACF and PACF.

```{r}
ggplot() +
  geom_line(aes(y=P, x=seq(1,length(P))))
```

ACF and PACF analysis shows an exponential decay for the ACF and a single peek at log 1 in the PACF this suggests that the distribution is stationary. However this hypothesis is yet to be confirmed using statistical tests.

```{r}
acf(P, lag.max = 100)
pacf(P, lag.max = 10)
```

In order to confirm the stationary we can use the KPSS test and the ADF test.

```{r}
kpss.test(P, null = "Trend")
adf.test(P)
```

Given the p-values of the two tests, we can confirm that the Markov chain reached stationarity and that what we're observing is our tart distribution.

#### Effect of eps_p:

```{r}
T_accept <- 1000
P0_accept <- 0.9
source('metro_hasting_p.R')

ac_rate <- function(x){return(metro_hasting_p(P0_accept, x, T_accept)$acceptance_rate)}
X <- seq(0,10000, length.out = 1000)
Z <- rep(NA, length(X))
for (i in 1:length(Z)){ 
  Z[i] <- ac_rate(X[i])
}
```

```{r}
#Plot the acceptance rate
ggplot()+
  geom_line(aes(y=Z, x=X))+
  ggtitle("Evolution of the acceptance rate with respect to eps_p")+
  labs(y = "Acceptance rate", x="eps_p")
```

The acceptance rate grows exponentially with respect to eps_p.

We know run the sampling algorithm for 3 values of eps_p that yield different acceptances rates.

```{r}
T_accept = 1000
EPS <- c(0.1, 300, 10000)
PP <- matrix(NA, nrow = length(EPS), ncol = T_accept)
for (i in 1:length(EPS)){ PP[i,] <- metro_hasting_p(P0_accept, EPS[i], T_accept)$Sample}
```

```{r}
b <- 50 # Number of bins
hist1 <- ggplot(data.frame(x = PP[1,]), aes(x = x))+
  geom_histogram(bins = b, fill = "red")+
  ggtitle(sprintf("eps_p = 0.1\nacceptance = %.3f", ac_rate(0.1)))
hist2 <- ggplot(data.frame(x = PP[2,]), aes(x = x))+
  geom_histogram(bins = b, fill = "darkgreen")+
  ggtitle(sprintf("eps_p = 300\nacceptance = %.3f", ac_rate(300)))
hist3 <- ggplot(data.frame(x = PP[3,]), aes(x = x))+
  geom_histogram(bins = b, fill = "blue")+
  ggtitle(sprintf("eps_p = 10000\nacceptance = %.3f", ac_rate(10000)))
grid.arrange(hist1, hist2, hist3, ncol = 3, top = textGrob("Histograms for different values of eps_p"))
```

Through that the higher the acceptance rate the more likely the sampler is to travel in the support of the posterior beta distribution.

#### Sensitivity to starting conditions:

```{r}
eps_p <- 300
P_0 <- runif(10)
PP_ <- metro_hasting_p(P_0, eps_p , T)$Sample
```

```{r}
df <- as.data.frame(PP_[1:150,])
col_names <- sprintf(paste0("%.", 3, "f"), P_0)
names(df) <- col_names
df_melted <- melt(df, id.vars = NULL)
df_melted$x <- rep(1:150, ncol(df))
ggplot(data = df_melted, aes(x = x, y = value, color = variable)) +
  geom_line()+
  ggtitle("MCMC iterations for different starting points")
```

The graph above shows that the algorithm converges to the stationary distribution in less than a 100 steps. This observation leads us to studying the convergence rate of the algorithm.

## [Sampling the $\eta_1$ , $\eta_2$ , and $\varphi$ :]{.underline}

We know sample the parameters $\eta_1$ , $\eta_2$, and $\varphi$ .

##### Prior distribution:

$$
(\eta_1, \eta_2, \varphi) \sim \mathcal{D}(\alpha_1, \alpha_2, \alpha_3)
$$

```{r}
library(DirichletReg)
Alpha <- c(.5,.5,.5)
```

##### Posterior distribution:

```{r}
source("Log_likelihood.R")
Log_post_etaPhi <- function (p, etaPhi, sigma, mu, Alpha, Y){
  return( Log_lik_etaPhi(p, etaPhi, sigma, mu, Y) + ddirichlet(matrix(etaPhi, nrow = 1), Alpha, log = TRUE))
}
```

##### Metropolis Hastings sampler : eta-phi

```{r}
# Number of iterations
T <- 50000
##------------- Sampling algorithm
#Constants of the algorithm
mu1_0 <- 1
mu2_0 <- 1
mu1 <- vector(mode = "numeric", length = T)
mu2 <- vector(mode = "numeric", length = T)
mu1[1] <- mu1_0
mu2[1] <- mu2_0

# Initialization etaPhi (contient les carres de eta_1, eta_2 et Phi )
eps_etaPhi <- 300
etaPhi_0 <- rdirichlet(1, Alpha)
ETAPHI <- matrix(NA, T, 3)
ETAPHI[1, ] <- etaPhi_0

# Initialisation sigma1/2
sigma1_0 <- 1
sigma2_0 <- 1
sigma1 <- vector(mode = "numeric", length = T)
sigma2 <- vector(mode = "numeric", length = T)
sigma1[1] <- sigma1_0
sigma2[1] <- sigma2_0

#Acceptance
Acceptance <- NULL
for (i in 1:(T - 1)) {
  # Generating a proposal
  etaPhi_t <- ETAPHI[i, ]
  etaPhi_prop <-
    rdirichlet(1,
               c(
                 etaPhi_t[1] * eps_etaPhi + 1,
                 etaPhi_t[2] * eps_etaPhi + 1,
                 etaPhi_t[3] * eps_etaPhi + 1
               ))
  # Computing acceptance probability
  C1 <-
    c(
      etaPhi_prop[1] * eps_etaPhi + 1,
      etaPhi_prop[2] * eps_etaPhi + 1,
      etaPhi_prop[3] * eps_etaPhi + 1
    )
  C2 <-
    c(etaPhi_t[1] * eps_etaPhi + 1,
      etaPhi_t[2] * eps_etaPhi + 1,
      etaPhi_t[3] * eps_etaPhi + 1)
  log_acceptance <-
    (
      Log_post_etaPhi(p, sqrt(etaPhi_prop), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_t, nrow = 1), C1 , log = TRUE)
    ) - (Log_post_etaPhi(p, sqrt(etaPhi_t), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_prop, nrow = 1), C2, log = TRUE))
  acc <- min(1, exp(log_acceptance))
  # Decision on the next step
  U = runif(1)
  if (acc > U)
    ETAPHI[i + 1, ] <- etaPhi_prop
  else
    ETAPHI[i + 1, ] <- etaPhi_t
  # Update sigma1/2
  ephi <- sqrt(ETAPHI[i+1,])
  sigma1[i + 1] <- sigma * (ephi[1] / sqrt(p))
  sigma2[i + 1] <- sigma * (ephi[2] / sqrt(1 - p))
  mu1[i + 1] <- mu - sigma * ephi[3] * (sqrt(1 - p) / sqrt(p))
  mu2[i + 1] <- mu + sigma * ephi[3] * (sqrt(p) / sqrt(1 - p))
  
  Acceptance <- c(Acceptance, acc) 
}

```

```{r}
n_trunc <- 100
mu1 <- mu1[n_trunc:length(mu1)]
mu2 <- mu2[n_trunc:length(mu2)]
sigma1 <- sigma1[n_trunc:length(sigma1)]
sigma2 <- sigma2[n_trunc:length(sigma2)]
```

### Results:

The plot of the ETAPHI values in the $S^2$ space shows that the value of the coordinates end up converging to a specific value. We can see this as a first proof of the convergence of the algorithm.

```{r}
source("plot_dirichlet.R")
plot_dirichlet(ETAPHI)
```

```{r warning=FALSE}
ggplot() +
  geom_histogram(aes(x=sigma1), bins = 300) +
  geom_vline(xintercept = sigma_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma1), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de sigma1\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")

ggplot() +
  geom_histogram(aes(x=sigma2), bins = 300) +
  geom_vline(xintercept = sigma_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma2), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de sigma2\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")
```

```{r}
ggplot() +
  geom_histogram(aes(x=mu1), bins = 300) +
  geom_vline(xintercept = mu_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu1), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de mu1\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")

ggplot() +
  geom_histogram(aes(x=mu2), bins = 300) +
  geom_vline(xintercept = mu_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu2), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de mu2\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")
```

#### Stationary of the Markov chain:

```{r}
acf(sigma1)
pacf(sigma1)
```

```{r}
acf(sigma2)
pacf(sigma2)
```

```{r}
acf(mu1)
pacf(mu1)
```

```{r}
acf(mu2)
pacf(mu2)
```

The acf and pacf plots for the simulated parameters have features that indicate a strong stationary which indicates with the low error of the estimators that the algorithm is convergent.

## Sampling $p$, [$\eta_1$ , $\eta_2$ , and $\varphi$ :]{.underline}

```{r}
source("Log_likelihood.R")

Log_post_p <- function (p, etaPhi, sigma, mu, Tau, Y) {
  return( Log_lik_etaPhi_p(p, etaPhi, sigma, mu, Y) + log(dbeta(p, Tau$tau1, Tau$tau2)))
}

Log_post_etaPhi <- function (p, etaPhi, sigma, mu, Alpha, Y){
  return( Log_lik_etaPhi_p(p, etaPhi, sigma, mu, Y) + ddirichlet(matrix(etaPhi, nrow = 1), Alpha, log = TRUE))
}
```

```{r}
library(DirichletReg)
mu <- mean(Y)
sigma <- sd(Y)
```

```{r}
# Number of iterations
T <- 10000
##------------- Sampling algorithm
#Initialize p
eps_p <- 3
P <- numeric(length = T)
P[1] <- .89

#Initialize mu1 and mu2
mu1 <- mu2 <- numeric(length = T)
mu1[1] <- 1
mu2[1] <- 1

# Initialize etaPhi_sqrd
eps_etaPhi_sqrd <- 300
etaPhi_sqrd <- matrix(NA, T, 3)
etaPhi_sqrd[1, ] <- rdirichlet(1, Alpha)

# Initialize sigma1 and sigma2
sigma1 <- sigma2 <- numeric(length = T)
sigma1[1] <- 1
sigma2[1] <- 1

#Acceptance
Acceptance_p <- NULL
Acceptance_etaPhi_sqrd <- NULL
acc_rate_p <- 0
acc_rate_etaPhi_sqrd <- 0
for (i in 1:(T - 1)) {
  # Initialize the Gibbs iteration
  p_t <- P[i]
  etaPhi_sqrd_t <- etaPhi_sqrd[i, ]
  
  ##=======================Sample the mixture weight=====================================>
    #Generate a proposal
    p_prop <- rbeta(1, (p_t*eps_p)+1, ((1-p_t)*eps_p)+1)
  
    # Computing acceptance
    log_acceptance <- 
      (Log_post_p(p_prop, etaPhi_sqrd_t, sigma, mu, Tau, Y) + dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1, log = TRUE)) -
      (Log_post_p(p_t, etaPhi_sqrd_t, sigma, mu, Tau, Y) + dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1, log = TRUE))
    acc_p <- min(1, exp(log_acceptance))
    
    # Decision on the next step 
    U = runif(1)
    if ( acc_p > U){
      P[i+1] <- p_prop
      acc_rate_p <- acc_rate_p + (1/T) 
    }
    else 
      P[i+1] <- p_t
    
    Acceptance_p <- c(Acceptance_p, acc_p)
  ##======================Sample the eta1, et2 and phi all squared========================>
    # Generate a proposal
    etaPhi_sqrd_prop <-
    rdirichlet(1,
               c(
                 etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
                 etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
                 etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1
               ))
    # Computing acceptance probability
    tmp1 <-
      c(
        etaPhi_sqrd_prop[1] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_prop[2] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_prop[3] * eps_etaPhi_sqrd + 1
      )
    tmp2 <-
      c(etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1)
    log_acceptance <-
      (
        Log_post_etaPhi(p, sqrt(etaPhi_sqrd_prop), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_t, nrow = 1), tmp1 , log = TRUE)
      ) - (Log_post_etaPhi(p, sqrt(etaPhi_sqrd_t), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_prop, nrow = 1), tmp2, log = TRUE))
    acc_etaPhi_sqrd <- min(1, exp(log_acceptance))
    # Decision
    U = runif(1)
    if (acc_etaPhi_sqrd > U){
      etaPhi_sqrd[i + 1, ] <- etaPhi_sqrd_prop
      acc_rate_etaPhi_sqrd <- acc_rate_etaPhi_sqrd + (1/T) 
    }
    else
      etaPhi_sqrd[i + 1, ] <- etaPhi_sqrd_t
    
    # Update sigma1/2 and mu1/2
    ephi <- sqrt(etaPhi_sqrd[i+1,])
    sigma1[i + 1] <- sigma * (ephi[1] / sqrt(p))
    sigma2[i + 1] <- sigma * (ephi[2] / sqrt(1 - p))
    mu1[i + 1] <- mu - sigma * ephi[3] * (sqrt(1 - p) / sqrt(p))
    mu2[i + 1] <- mu + sigma * ephi[3] * (sqrt(p) / sqrt(1 - p))
    
    Acceptance_etaPhi_sqrd <- c(Acceptance_etaPhi_sqrd, acc_etaPhi_sqrd) 
}

```

```{r warning=FALSE}
P <- P[n_trunc:length(P)]
ggplot() +
  geom_histogram(aes(x=P), bins = 1000) +
  geom_vline(xintercept = p, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(P), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de P\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")+
  xlim(0,1)
```

```{r}
sigma1 <- sigma1[n_trunc:length(sigma1)]
ggplot() +
  geom_histogram(aes(x=sigma1), bins = 200) +
  geom_vline(xintercept = sigma_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma1), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de sigma1\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")

sigma2 <- sigma2[n_trunc:length(sigma2)]
ggplot() +
  geom_histogram(aes(x=sigma2), bins = 200) +
  geom_vline(xintercept = sigma_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma2), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de sigma2\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")
```

```{r}
mu1 <- mu1[n_trunc:length(mu1)]
ggplot() +
  geom_histogram(aes(x=mu1), bins = 1000) +
  geom_vline(xintercept = mu_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu1), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de mu1\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")

mu2 <- mu2[n_trunc:length(mu2)]
ggplot() +
  geom_histogram(aes(x=mu2), bins = 1000) +
  geom_vline(xintercept = mu_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu2), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de mu2\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")
```
