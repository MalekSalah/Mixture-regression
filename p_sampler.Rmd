---
title: "Sampling algorithm"
output:
  html_document:
---

```{r}
library(ggplot2)
library(tseries)
library(gridExtra)
library(grid)
```

#### [The model:]{.underline}

We suppose we have an n-sample of the random variable $Y$ and want to estimate the mixture parameter using a Bayesian approach.

$$
Y|\mu_1,\sigma_1,\mu_2,\sigma_2,p\sim p \mathcal{N}(\mu_1,\sigma_1)+(1-p) \mathcal{N}(\mu_2,\sigma_2).
$$

```{r}
p <- 0.7 #Mixture weight

# Component parameters
sigma_1 <- 1 
sigma_2 <- 1
mu_1 <- 4 
mu_2 <- 0

Theta1 = list("mu"=mu_1, "sigma"= sigma_1)
Theta2 = list("mu"=mu_2, "sigma"= sigma_2)
```

we use a mixture sampling algorithm to create our sample

```{r echo=TRUE}
# Number of samples from the mixture distribution
N <- 3000
# Sample N uniforms U
B <- sample(1:2, size = N, replace = TRUE, prob = c(p, 1-p))

# Sampling from the mixture
Mu <- c(mu_1, mu_2)
Std <- c(sigma_1, sigma_2)
Y <- rnorm(N, mean = Mu[B], sd = Std[B])

# Plotting the distributions
ggplot() + 
  geom_histogram(aes(Y), bins = 100)+
  ggtitle("Historgram of the mixture distribution Y")
```

#### [Sampling the mixture weight p :]{.underline}

In this section we suppose that there is only one parameter of interest and suppose all the others constants equal to some arbitrary values.

##### Prior distribution:

The parameter $p$ lies on $[0,1]$ which suggests a Beta like distribution for the prior

$$
p \sim  \mathcal{B}(\tau_0, \tau_1)
$$

```{r}
Tau <- list("tau1"=0.5 , "tau2"=0.5)
```

##### The likelihood:

In the case of our model, the likelihood function is given by

$$
\ell(\Theta|y,X) = \prod_{i=1}^{n} \frac{p}{\sqrt{2\pi}\sigma_1} \exp \left( -\frac{(y_i-\mu_1)^2}{2\sigma_1^2} \right) + \frac{1-p}{\sqrt{2\pi}\sigma_2} \exp \left( -\frac{(y_i-\mu_2)^2}{2\sigma_2^2} \right).
$$

```{r}
Log_lik <- function (p, Theta1, Theta2, Y){
  res <- vector(mode = "numeric", length(p))
  for (i in 1:length(p)){
    res[i] <- sum(log(p[i] * dnorm(Y, Theta1$mu, Theta1$sigma) + (1-p[i]) * dnorm(Y, Theta2$mu, Theta2$sigma)))
  }
  return(res)
}
```

##### Posterior distribution:

The posterior distribution we want to sample from is

$$
\pi(p \mid \Theta,y,X) = \frac{\ell(\Theta|y,X)}{B(\tau_0, \tau_1)} p^{\tau_0-1}(1-p)^{\tau_1-1}, \qquad B(\tau_0, \tau_1) = \frac{\Gamma(\tau_0)\Gamma(\tau_1)}{\Gamma(\tau_0+\tau_1)} 
$$

```{r}
Log_post_p <- function (p, Theta1, Theta2, Tau, Y) {
  return(Log_lik(p, Theta1, Theta2, Y) + log(dbeta(p, Tau$tau1, Tau$tau2)))
}
```

##### Metropolis-Hastings sampler:

```{r}
# Number of iterations
T <- 50000

##------------- Sampling algorithm
# Initialization
eps_p <- 0.4
p_0 <- 0.9
P <- p_0
Acceptance <- NULL


for (i in 1:T){    
  # Generating a proposal
  p_t <- P[length(P)]
  p_prop <- rbeta(1, p_t*eps_p+1, (1-p_t)*eps_p+1)
  
  # Computing acceptance probability
  log_acceptance <- (Log_post_p(p_prop, Theta1, Theta2, Tau, Y)+log(dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1))) - (Log_post_p(p_t, Theta1, Theta2, Tau, Y) + log(dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1)))
  acc <- min(1, exp(log_acceptance))
  
  # Decision on the next step 
  U = runif(1)
  if ( acc > U) P <- c(P, p_prop) 
  else P <- c(P, p_t)
}
```

#### [Results:]{.underline}

```{r}
ggplot() +
  geom_line(aes(y=P, x=seq(1,length(P))))
```

The Markov chain constructed by the Metropolis-Hastings algorithm quickly starts to fluctuate around the exact value of the parameter. Since we're more interested in the stationary part of the chain, we can discard the first few elements and focus on the fluctuation around the exact value of $p$.

```{r}
n_trunc <- 100 # Index at which we start the truncated Markov Chain
P <- P[n_trunc:length(P)]
```

The histogram bellow shows the distribution of the obtained sample. One important observation is that the Bayesian estimator (in blue) is close to the exact value of the parameter (in red).

```{r warning=FALSE}
ggplot() +
  geom_histogram(aes(x=P), bins = 1000) +
  geom_vline(xintercept = p, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(P), color="blue", show.legend = TRUE) +
  ggtitle("Histogramme des echantillons de P\n En rouge la valeur exacte, en bleu l'estimateur de Bayes")+
  xlim(0,1)

```

The obtained distribution is a beta distribution with very high parameters which explains the peaked value around the mean.

#### Stationary of the Markov chain

We now try to asses the stationary of our Markov chain by studying the autocorrelation and performing both ACF and PACF.

```{r}
ggplot() +
  geom_line(aes(y=P, x=seq(1,length(P))))
```

ACF and PACF analysis shows an exponential decay for the ACF and a single peek at log 1 in the PACF this suggests that the distribution is stationary. However this hypothesis is yet to be confirmed using statistical tests.

```{r}
acf(P, lag.max = 100)
pacf(P, lag.max = 10)
```

In order to confirm the stationary we can use the KPSS test and the ADF test.

```{r}
kpss.test(P, null = "Trend")
adf.test(P)
```

Given the p-values of the two tests, we can confirm that the Markov chain reached stationarity and that what we're observing is our targt distribution.

#### Effect of eps_p:

```{r}
T_accept <- 1000
P0_accept <- 0.9
source('metro_hasting_p.R')

ac_rate <- function(x){return(metro_hasting_p(P0_accept, x, T_accept)$acceptance_rate)}
X <- seq(0,10000, length.out = 1000)
Z <- rep(NA, length(X))
for (i in 1:length(Z)){ 
  Z[i] <- ac_rate(X[i])
}
```

```{r}
#Plot the acceptance rate
ggplot()+
  geom_line(aes(y=Z, x=X))+
  ggtitle("Evolution of the acceptance rate with respect to eps_p")+
  labs(y = "Acceptance rate", x="eps_p")
```

The acceptance rate grows exponentially with respect to eps_p.

We know run the sampling algorithm for 3 values of eps_p that yield different acceptances rates.

```{r}
T_accept = 1000
EPS <- c(0.1, 300, 10000)
PP <- matrix(NA, nrow = length(EPS), ncol = T_accept)
for (i in 1:length(EPS)){ PP[i,] <- metro_hasting_p(P0_accept, EPS[i], T_accept)$Sample}
```

```{r}
b <- 50 # Number of bins
hist1 <- ggplot(data.frame(x = PP[1,]), aes(x = x))+
  geom_histogram(bins = b, fill = "red")+
  ggtitle(sprintf("eps_p = 0.1\nacceptance = %.3f", ac_rate(0.1)))
hist2 <- ggplot(data.frame(x = PP[2,]), aes(x = x))+
  geom_histogram(bins = b, fill = "darkgreen")+
  ggtitle(sprintf("eps_p = 300\nacceptance = %.3f", ac_rate(300)))
hist3 <- ggplot(data.frame(x = PP[3,]), aes(x = x))+
  geom_histogram(bins = b, fill = "blue")+
  ggtitle(sprintf("eps_p = 10000\nacceptance = %.3f", ac_rate(10000)))
grid.arrange(hist1, hist2, hist3, ncol = 3, top = textGrob("Histograms for different values of eps_p"))
```

Through that the higher the acceptance rate the more likely the sampler is to travel in the support of the posterior beta distribution.
