---
---
---

```{r warning=FALSE}
library(ggplot2)
library(reshape2)
library(tseries)
library(gridExtra)
library(grid)
library(dplyr)
library(invgamma)
```

# [The model:]{.underline}

We suppose we have an n-sample of the random variable $Y$ and want to estimate the mixture parameter using a Bayesian approach.$$
Y|\mu_1,\sigma_1,\mu_2,\sigma_2,p\sim p \mathcal{N}(\mu_1,\sigma_1)+(1-p) \mathcal{N}(\mu_2,\sigma_2).
$$

```{r}
p <- 0.2 #Mixture weight

# Component parameters
sigma_1 <- 1
sigma_2 <- 2
mu_1 <- 2 
mu_2 <- 10

Theta1 = list("mu"=mu_1, "sigma"= sigma_1)
Theta2 = list("mu"=mu_2, "sigma"= sigma_2)
```

we use a mixture sampling algorithm sample from

$$
Y \sim 0.2\mathcal{N}(2,1)+0.8\mathcal{N}(10,2)
$$

```{r echo=TRUE}
source("rnornix.R")
# Number of samples from the mixture distribution
N <- 3000

# Sampling from the mixture
Mix <- rnormix(N, p, mu_1, sigma_1, mu_2, sigma_2)
Y <- Mix$samples
mu <- Mix$mean
sigma <- Mix$sd


# Plotting the distributions
ggplot() + 
  geom_histogram(aes(Y), bins = 100)+
  ggtitle("Historgram of the mixture distribution Y")
```

## [Sampling the mixture weight p :]{.underline}

In this section we suppose that there is only one parameter of interest and suppose all the others constants equal to some arbitrary values.

##### Prior distribution:

The parameter $p$ lies on $[0,1]$ which suggests a Beta like distribution for the prior

$$
p \sim  \mathcal{B}(\tau_0, \tau_1)
$$

```{r}
Tau <- list("tau1"=0.5 , "tau2"=0.5)
```

##### The likelihood:

In the case of our model, the likelihood function is given by

$$
\ell(\Theta|y,X) = \prod_{i=1}^{n} \frac{p}{\sqrt{2\pi}\sigma_1} \exp \left( -\frac{(y_i-\mu_1)^2}{2\sigma_1^2} \right) + \frac{1-p}{\sqrt{2\pi}\sigma_2} \exp \left( -\frac{(y_i-\mu_2)^2}{2\sigma_2^2} \right).
$$

```{r}
Log_lik <- function (p, Theta1, Theta2, Y){
  res <- vector(mode = "numeric", length(p))
  for (i in 1:length(p)){
    res[i] <- sum(log(p[i] * dnorm(Y, Theta1$mu, Theta1$sigma) + (1-p[i]) * dnorm(Y, Theta2$mu, Theta2$sigma)))
  }
  return(res)
}
```

##### Posterior distribution:

The posterior distribution we want to sample from is

$$
\pi(p \mid \Theta,y,X) = \frac{\ell(\Theta|y,X)}{B(\tau_0, \tau_1)} p^{\tau_0-1}(1-p)^{\tau_1-1}, \qquad B(\tau_0, \tau_1) = \frac{\Gamma(\tau_0)\Gamma(\tau_1)}{\Gamma(\tau_0+\tau_1)} 
$$

```{r}
Log_post_p <- function (p, Theta1, Theta2, Tau, Y) {
  return(Log_lik(p, Theta1, Theta2, Y) + log(dbeta(p, Tau$tau1, Tau$tau2)))
}
```

##### Metropolis-Hastings sampler:

```{r}
# Number of iterations
T <- 100000

##------------- Sampling algorithm
# Initialization
eps_p <- 0.4
p_0 <- 0.9
P <- p_0
Acceptance <- NULL


for (i in 1:T){    
  # Generating a proposal
  p_t <- P[length(P)]
  p_prop <- rbeta(1, p_t*eps_p+1, (1-p_t)*eps_p+1)
  
  # Computing acceptance probability
  log_acceptance <- (Log_post_p(p_prop, Theta1, Theta2, Tau, Y)+log(dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1))) - (Log_post_p(p_t, Theta1, Theta2, Tau, Y) + log(dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1)))
  acc <- min(1, exp(log_acceptance))
  
  # Decision on the next step 
  U = runif(1)
  if ( acc > U) P <- c(P, p_prop) 
  else P <- c(P, p_t)
}
```

### [Results:]{.underline}

```{r}
ggplot() +
  geom_line(aes(y=P, x=seq(1,length(P))))+
  xlab("iteration")
```

The Markov chain constructed by the Metropolis-Hastings algorithm quickly starts to fluctuate around the exact value of the parameter. Since we're more interested in the stationary part of the chain, we can discard the first few elements and focus on the fluctuation around the exact value of $p$.

```{r}
n_trunc <- 100 # Index at which we start the truncated Markov Chain
P <- P[n_trunc:length(P)]
```

The histogram bellow shows the distribution of the obtained sample. One important observation is that the Bayesian estimator (in blue) is close to the exact value of the parameter (in red).

```{r warning=FALSE}
ggplot() +
  geom_histogram(aes(x=P), bins = 200) +
  geom_vline(xintercept = p, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(P), color="blue", show.legend = TRUE)+
  ggtitle(sprintf("err=%.6f", abs(mean(P)-p)))

```

The obtained distribution is a beta distribution with very high parameters which explains the peaked value around the mean.

#### Stationary of the Markov chain

We now try to asses the stationary of our Markov chain by studying the autocorrelation and performing both ACF and PACF.

```{r}
ggplot() +
  geom_line(aes(y=P, x=seq(1,length(P))))
```

ACF and PACF analysis shows an exponential decay for the ACF and a single peek at log 1 in the PACF this suggests that the distribution is stationary. However this hypothesis is yet to be confirmed using statistical tests.

```{r}
acf(P, lag.max = 100)
pacf(P, lag.max = 10)
```

In order to confirm the stationary we can use the KPSS test and the ADF test.

```{r}
kpss.test(P)
adf.test(P)
PP.test(P)
```

Given the p-values of the two tests, we can confirm that the Markov chain reached stationarity and that what we're observing is our tart distribution.

#### Effect of eps_p:

```{r}
T_accept <- 1000
P0_accept <- 0.9
source("metro_hasting_p.R")

ac_rate <- function(x){return(metro_hasting_p(P0_accept, x, T_accept)$acceptance_rate)}
X <- seq(0,10000, length.out = 1000)
Z <- rep(NA, length(X))
for (i in 1:length(Z)){ 
  Z[i] <- ac_rate(X[i])
}
```

```{r}
#Plot the acceptance rate
ggplot()+
  geom_line(aes(y=Z, x=X))+
  ggtitle("Evolution of the acceptance rate with respect to eps_p")+
  labs(y = "Acceptance rate", x="eps_p")
```

The acceptance rate grows exponentially with respect to eps_p.

We know run the sampling algorithm for 3 values of eps_p that yield different acceptances rates.

```{r}
T_accept = 1000
EPS <- c(0.1, 300, 10000)
PP <- matrix(NA, nrow = length(EPS), ncol = T_accept)
for (i in 1:length(EPS)){ PP[i,] <- metro_hasting_p(P0_accept, EPS[i], T_accept)$Sample}
```

```{r}
b <- 50 # Number of bins
hist1 <- ggplot(data.frame(x = PP[1,]), aes(x = x))+
  geom_histogram(bins = b, fill = "red")+
  ggtitle(sprintf("eps_p = 0.1\nacceptance = %.3f", ac_rate(0.1)))
hist2 <- ggplot(data.frame(x = PP[2,]), aes(x = x))+
  geom_histogram(bins = b, fill = "darkgreen")+
  ggtitle(sprintf("eps_p = 300\nacceptance = %.3f", ac_rate(300)))
hist3 <- ggplot(data.frame(x = PP[3,]), aes(x = x))+
  geom_histogram(bins = b, fill = "blue")+
  ggtitle(sprintf("eps_p = 10000\nacceptance = %.3f", ac_rate(10000)))
grid.arrange(hist1, hist2, hist3, ncol = 3, top = textGrob("Histograms for different values of eps_p"))
```

The higher the acceptance rate is, the more likely the sampler is to travel in the support of the posterior beta distribution.

#### Sensitivity to starting conditions:

```{r}
eps_p <- 300
P_0 <- runif(10)
PP_ <- metro_hasting_p(P_0, eps_p , T)$Sample
```

```{r}
df <- as.data.frame(PP_[1:150,])
col_names <- sprintf(paste0("%.", 3, "f"), P_0)
names(df) <- col_names
df_melted <- melt(df, id.vars = NULL)
df_melted$x <- rep(1:150, ncol(df))
ggplot(data = df_melted, aes(x = x, y = value, color = variable)) +
  geom_line()+
  ggtitle("MCMC iterations for different starting points")
```

The graph above shows that the algorithm converges to the stationary distribution in less than a 100 steps. This observation leads us to studying the convergence rate of the algorithm.

## [Sampling the $\eta_1$ , $\eta_2$ , and $\varphi$ :]{.underline}

We know sample the parameters $\eta_1$ , $\eta_2$, and $\varphi$ .

##### Prior distribution:

$$
(\eta_1, \eta_2, \varphi) \sim \mathcal{D}(\alpha_1, \alpha_2, \alpha_3)
$$

```{r}
library(DirichletReg)
Alpha <- c(.5,.5,.5)
```

##### Posterior distribution:

```{r}
source("Log_likelihood.R")
Log_post_etaPhi <- function (p, etaPhi, sigma, mu, Alpha, Y){
  return( Log_lik_etaPhi_p(p, etaPhi, sigma, mu, Y) + ddirichlet(matrix(etaPhi, nrow = 1), Alpha, log = TRUE))
}
```

##### Metropolis Hastings sampler : eta-phi

```{r}
# Number of iterations
T <- 50000
##------------- Sampling algorithm
#Constants of the algorithm
mu1_0 <- 1
mu2_0 <- 1
mu1 <- vector(mode = "numeric", length = T)
mu2 <- vector(mode = "numeric", length = T)
mu1[1] <- mu1_0
mu2[1] <- mu2_0

# Initialization etaPhi (contient les carres de eta_1, eta_2 et Phi )
eps_etaPhi <- 300
etaPhi_0 <- rdirichlet(1, Alpha)
ETAPHI <- matrix(NA, T, 3)
ETAPHI[1, ] <- etaPhi_0

# Initialisation sigma1/2
sigma1_0 <- 1
sigma2_0 <- 1
sigma1 <- vector(mode = "numeric", length = T)
sigma2 <- vector(mode = "numeric", length = T)
sigma1[1] <- sigma1_0
sigma2[1] <- sigma2_0

#Acceptance
Acceptance <- NULL
for (i in 1:(T - 1)) {
  # Generating a proposal
  etaPhi_t <- ETAPHI[i, ]
  etaPhi_prop <-
    rdirichlet(1,
               c(
                 etaPhi_t[1] * eps_etaPhi + 1,
                 etaPhi_t[2] * eps_etaPhi + 1,
                 etaPhi_t[3] * eps_etaPhi + 1
               ))
  # Computing acceptance probability
  C1 <-
    c(
      etaPhi_prop[1] * eps_etaPhi + 1,
      etaPhi_prop[2] * eps_etaPhi + 1,
      etaPhi_prop[3] * eps_etaPhi + 1
    )
  C2 <-
    c(etaPhi_t[1] * eps_etaPhi + 1,
      etaPhi_t[2] * eps_etaPhi + 1,
      etaPhi_t[3] * eps_etaPhi + 1)
  log_acceptance <-
    (
      Log_post_etaPhi(p, sqrt(etaPhi_prop), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_t, nrow = 1), C1 , log = TRUE)
    ) - (Log_post_etaPhi(p, sqrt(etaPhi_t), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_prop, nrow = 1), C2, log = TRUE))
  acc <- min(1, exp(log_acceptance))
  # Decision on the next step
  U = runif(1)
  if (acc > U)
    ETAPHI[i + 1, ] <- etaPhi_prop
  else
    ETAPHI[i + 1, ] <- etaPhi_t
  # Update sigma1/2
  ephi <- sqrt(ETAPHI[i+1,])
  sigma1[i + 1] <- sigma * (ephi[1] / sqrt(p))
  sigma2[i + 1] <- sigma * (ephi[2] / sqrt(1 - p))
  mu1[i + 1] <- mu - sigma * ephi[3] * (sqrt(1 - p) / sqrt(p))
  mu2[i + 1] <- mu + sigma * ephi[3] * (sqrt(p) / sqrt(1 - p))
  
  Acceptance <- c(Acceptance, acc) 
}

```

```{r}
n_trunc <- 100
mu1 <- mu1[n_trunc:length(mu1)]
mu2 <- mu2[n_trunc:length(mu2)]
sigma1 <- sigma1[n_trunc:length(sigma1)]
sigma2 <- sigma2[n_trunc:length(sigma2)]
```

### Results:

The plot of the ETAPHI values in the $S^2$ space shows that the value of the coordinates end up converging to a specific value. We can see this as a first proof of the convergence of the algorithm.

```{r}
source("plot_dirichlet.R")
plot_dirichlet(ETAPHI)
```

```{r warning=FALSE}
ggplot() +
  geom_histogram(aes(x=sigma1), bins = 300) +
  geom_vline(xintercept = sigma_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma1), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(sigma1)-sigma_1)))

ggplot() +
  geom_histogram(aes(x=sigma2), bins = 300) +
  geom_vline(xintercept = sigma_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma2), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(sigma2)-sigma_2)))
```

```{r}
ggplot() +
  geom_histogram(aes(x=mu1), bins = 300) +
  geom_vline(xintercept = mu_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu1), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(mu1)-mu_1)))

ggplot() +
  geom_histogram(aes(x=mu2), bins = 300) +
  geom_vline(xintercept = mu_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu2), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(mu2)-mu_2)))
```

#### Stationary of the Markov chain:

```{r}
acf(sigma1)
pacf(sigma1)
```

```{r}
acf(sigma2)
pacf(sigma2)
```

```{r}
acf(mu1)
pacf(mu1)
```

```{r}
acf(mu2)
pacf(mu2)
```

The acf and pacf plots for the simulated parameters have features that indicate a strong stationary which indicates with the low error of the estimators that the algorithm is convergent.

## Sampling $p$, [$\eta_1$ , $\eta_2$ , and $\varphi$ :]{.underline}

```{r}
source("Log_likelihood.R")

Log_post_p <- function (p, etaPhi, sigma, mu, Tau, Y) {
  return( Log_lik_etaPhi_p(p, etaPhi, sigma, mu, Y) + log(dbeta(p, Tau$tau1, Tau$tau2)))
}

Log_post_etaPhi <- function (p, etaPhi, sigma, mu, Alpha, Y){
  return( Log_lik_etaPhi_p(p, etaPhi, sigma, mu, Y) + ddirichlet(matrix(etaPhi, nrow = 1), Alpha, log = TRUE))
}


```

```{r}
library(DirichletReg)
```

```{r}
# Number of iterations
T <- 10000
##------------- Sampling algorithm
#Initialize p
eps_p <- 3
P <- numeric(length = T)
P[1] <- .89

#Initialize mu1 and mu2
mu1 <- mu2 <- numeric(length = T)
mu1[1] <- 1
mu2[1] <- 1

# Initialize etaPhi_sqrd
eps_etaPhi_sqrd <- 300
etaPhi_sqrd <- matrix(NA, T, 3)
etaPhi_sqrd[1, ] <- rdirichlet(1, Alpha)

# Initialize sigma1 and sigma2
sigma1 <- sigma2 <- numeric(length = T)
sigma1[1] <- 1
sigma2[1] <- 1

#Acceptance
Acceptance_p <- NULL
Acceptance_etaPhi_sqrd <- NULL
acc_rate_p <- 0
acc_rate_etaPhi_sqrd <- 0
for (i in 1:(T - 1)) {
  # Initialize the Gibbs iteration
  p_t <- P[i]
  etaPhi_sqrd_t <- etaPhi_sqrd[i, ]
  
  ##=======================Sample the mixture weight=====================================>
    #Generate a proposal
    p_prop <- rbeta(1, (p_t*eps_p)+1, ((1-p_t)*eps_p)+1)
    
    # Computing acceptance
    log_acceptance <- 
      (Log_post_p(p_prop, sqrt(etaPhi_sqrd_t), sigma, mu, Tau, Y) + dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1, log = TRUE)) -
      (Log_post_p(p_t, sqrt(etaPhi_sqrd_t), sigma, mu, Tau, Y) + dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1, log = TRUE))
    acc_p <- min(1, exp(log_acceptance))
    
    # Decision on the next step 
    U = runif(1)
    if ( acc_p > U){
      P[i+1] <- p_prop
      acc_rate_p <- acc_rate_p + (1/T) 
    }
    else 
      P[i+1] <- p_t
    
    Acceptance_p <- c(Acceptance_p, acc_p)
  ##======================Sample the eta1, et2 and phi all squared========================>
    # Generate a proposal
    etaPhi_sqrd_prop <-
    rdirichlet(1,
               c(
                 etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
                 etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
                 etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1
               ))
    # Computing acceptance probability
    tmp1 <-
      c(
        etaPhi_sqrd_prop[1] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_prop[2] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_prop[3] * eps_etaPhi_sqrd + 1
      )
    tmp2 <-
      c(etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1)
    log_acceptance <-
      (
        Log_post_etaPhi(p, sqrt(etaPhi_sqrd_prop), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_t, nrow = 1), tmp1 , log = TRUE)
      ) - (Log_post_etaPhi(p, sqrt(etaPhi_sqrd_t), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_prop, nrow = 1), tmp2, log = TRUE))
    acc_etaPhi_sqrd <- min(1, exp(log_acceptance))
    # Decision
    U = runif(1)
    if (acc_etaPhi_sqrd > U){
      etaPhi_sqrd[i + 1, ] <- etaPhi_sqrd_prop
      acc_rate_etaPhi_sqrd <- acc_rate_etaPhi_sqrd + (1/T) 
    }
    else
      etaPhi_sqrd[i + 1, ] <- etaPhi_sqrd_t
    
    # Update sigma1/2 and mu1/2
    ephi <- sqrt(etaPhi_sqrd[i+1,])
    sigma1[i + 1] <- sigma * (ephi[1] / sqrt(p))
    sigma2[i + 1] <- sigma * (ephi[2] / sqrt(1 - p))
    mu1[i + 1] <- mu - sigma * ephi[3] * (sqrt(1 - p) / sqrt(p))
    mu2[i + 1] <- mu + sigma * ephi[3] * (sqrt(p) / sqrt(1 - p))
    
    Acceptance_etaPhi_sqrd <- c(Acceptance_etaPhi_sqrd, acc_etaPhi_sqrd) 
}

```

```{r warning=FALSE}
P <- P[n_trunc:length(P)]
ggplot() +
  geom_histogram(aes(x=P), bins = 100) +
  geom_vline(xintercept = p, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(P), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(P)-p)))
```

```{r}
sigma1 <- sigma1[n_trunc:length(sigma1)]
ggplot() +
  geom_histogram(aes(x=sigma1), bins = 100) +
  geom_vline(xintercept = sigma_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma1), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(sigma1)-sigma_1)))

sigma2 <- sigma2[n_trunc:length(sigma2)]
ggplot() +
  geom_histogram(aes(x=sigma2), bins = 100) +
  geom_vline(xintercept = sigma_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma2), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(sigma2)-sigma_2)))
```

```{r}
mu1 <- mu1[n_trunc:length(mu1)]
ggplot() +
  geom_histogram(aes(x=mu1), bins = 100) +
  geom_vline(xintercept = mu_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu1), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(mu1)-mu_1)))

mu2 <- mu2[n_trunc:length(mu2)]
ggplot() +
  geom_histogram(aes(x=mu2), bins = 100) +
  geom_vline(xintercept = mu_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu2), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(mu2)-mu_2)))
```

## Sampling $p$ with acceptance rate adaptation:

```{r}
Log_lik <- function (p, Theta1, Theta2, Y){
  res <- vector(mode = "numeric", length(p))
  for (i in 1:length(p)){
    res[i] <- sum(log(p[i] * dnorm(Y, Theta1$mu, Theta1$sigma) + (1-p[i]) * dnorm(Y, Theta2$mu, Theta2$sigma)))
  }
  return(res)
}

Log_post_p <- function (p, Theta1, Theta2, Tau, Y) {
  return(Log_lik(p, Theta1, Theta2, Y) + log(dbeta(p, Tau$tau1, Tau$tau2)))
}
```

```{r}
# Number of iterations
T <- 10000
T_adapt <- 100


##------------- Sampling algorithm
# Initialization
eps_p <- 0.4
P <- numeric(length = T)
p_0 <- 0.8
P[1] <- p_0
P_adapt <- numeric(length = T_adapt)

# Algorithm vitals
ac_p <-  logical(length = T-1)
ac_rate_p <- numeric(length = T-1)

# Adaptation
ac_rate_lower <- .3
ac_rate_upper <- .35
adapt_eps_p <- FALSE
cur_adapt <- 200
ac_rate_span <- 20


for (i in 1:(T-1)){
  # Generating a proposal
  p_t <- P[i]
  p_prop <- rbeta(1, p_t*eps_p+1, (1-p_t)*eps_p+1)
  
  # Computing acceptance probability
  log_acceptance <- (Log_post_p(p_prop, Theta1, Theta2, Tau, Y)+log(dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1))) - (Log_post_p(p_t, Theta1, Theta2, Tau, Y) + log(dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1)))
  acc <- min(1, exp(log_acceptance))
  
  # Decision on the next step 
  U = runif(1)
  if (s <- (acc>U)) p_next <- p_prop
  else p_next <- p_t
  
  # Acceptance rate
  if (i==1) ac_p[i] <- s
  else ac_p[i] <- ac_p[i-1]+s
  ac_rate_p[i] <- ac_p[i]/i
  
  if ((i%%cur_adapt==0) && ((mean(ac_rate_p[(i-cur_adapt/2):i])>.4) || (mean(ac_rate_p[(i-cur_adapt/2):i])<.3))) {
    i_start <- i
    adapt_eps_p <- TRUE
    ac_rate_window <- numeric(ac_rate_span)
  }

  if (adapt_eps_p == TRUE){
    P_adapt[1] <- p_next
    for (j in 1:T_adapt){
      p_t <- P_adapt[j]
      p_prop <- rbeta(1, p_t*eps_p+1, (1-p_t)*eps_p+1)

      # Computing acceptance probability
      log_acceptance <- (Log_post_p(p_prop, Theta1, Theta2, Tau, Y)+log(dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1))) - (Log_post_p(p_t, Theta1, Theta2, Tau, Y) + log(dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1)))
        acc <- min(1, exp(log_acceptance))

      # Decision on the next step
      U = runif(1)
      if (s <- (acc>U)) P_adapt[j+1] <- p_prop
      else P_adapt[j+1] <- p_t

      ac_rate_window[j %% ac_rate_span + 1] <- s

      if (j >= ac_rate_span){
        current_ac_rate <- sum(ac_rate_window) / ac_rate_span

        # Adjust the eps_p
        if (current_ac_rate < ac_rate_lower) eps_p <- eps_p*1.1
        else if (current_ac_rate > ac_rate_upper) eps_p <- eps_p*0.9
      }
    }
    adapt_eps_p <- FALSE
  }
  P[i+1] <- p_next
}
```

```{r}
ggplot() +
  geom_line(aes(y=ac_rate_p, x=seq(1,length(ac_rate_p))))+
  geom_hline(yintercept = ac_rate_lower, linetype ="dashed", color  = "blue")+
  geom_hline(yintercept = ac_rate_upper, linetype ="dashed", color  = "blue")+
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin= ac_rate_lower, ymax=ac_rate_upper), alpha = .1, fill = "blue")+
  ggtitle("Evolution of the acceptance rate")+
  labs(y="Acceptance rate", x="Iteration number")
  
  
```

```{r warning=FALSE}
n_trunc <- 100
P <- P[n_trunc:length(P)]
ggplot() +
  geom_histogram(aes(x=P), bins = 100) +
  geom_vline(xintercept = p, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(P), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(P)-p)))
```

## Sampling etaPhi with adaptation :

```{r}
source("Log_likelihood.R")
Log_post_etaPhi <- function (p, etaPhi, sigma, mu, Alpha, Y){
  return( Log_lik_etaPhi_p(p, etaPhi, sigma, mu, Y) + ddirichlet(matrix(etaPhi, nrow = 1), Alpha, log = TRUE))
}

mu <- p*mu_1+(1-p)*mu_2
sigma <- sqrt(p*((sigma_1^2)+(mu_1^2)) + (1-p)*((sigma_2^2)+ (mu_2^2)) - (mu^2))

library(DirichletReg)
Alpha <- c(.5,.5,.5)
```

```{r}
# Number of iterations
T <- 20000
T_adapt <- 300

#Constants of the algorithm
mu1_0 <- 1
mu2_0 <- 1
mu1 <- vector(mode = "numeric", length = T)
mu2 <- vector(mode = "numeric", length = T)
mu1[1] <- mu1_0
mu2[1] <- mu2_0

# Algorithm vitals
ac_etaPhi_sqrd <-  logical(length = T-1)
ac_rate_etaPhi_sqrd <- numeric(length = T-1)

# Initialization etaPhi (contient les carres de eta_1, eta_2 et Phi )
eps_etaPhi_sqrd <- 300
etaPhi_sqrd_0 <- rdirichlet(1, Alpha)
etaPhi_sqrd <- matrix(NA, T, 3)
etaPhi_sqrd[1, ] <- etaPhi_sqrd_0
etaPhi_sqrd_adapt <- matrix(NA, T_adapt, 3)


# Initialisation sigma1/2
sigma1_0 <- 1
sigma2_0 <- 1
sigma1 <- vector(mode = "numeric", length = T)
sigma2 <- vector(mode = "numeric", length = T)
sigma1[1] <- sigma1_0
sigma2[1] <- sigma2_0

# Adaptation
ac_rate_lower <- .3
ac_rate_upper <- .35
adapt_eps_etaPhi_sqrd <- FALSE
cur_adapt <- 100
ac_rate_span <- 50
nb_adapt <- NULL


for (i in 1:(T - 1)) {
  # Generating a proposal
  etaPhi_sqrd_t <- etaPhi_sqrd[i, ]
  etaPhi_sqrd_prop <-
    rdirichlet(1,
               c(
                 etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
                 etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
                 etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1
               ))
  # Computing acceptance probability
  C1 <-
    c(
      etaPhi_sqrd_prop[1] * eps_etaPhi_sqrd + 1,
      etaPhi_sqrd_prop[2] * eps_etaPhi_sqrd + 1,
      etaPhi_sqrd_prop[3] * eps_etaPhi_sqrd + 1
    )
  C2 <-
    c(etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
      etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
      etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1)
  log_acceptance <-
    (
      Log_post_etaPhi(p, sqrt(etaPhi_sqrd_prop), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_t, nrow = 1), C1 , log = TRUE)
    ) - (Log_post_etaPhi(p, sqrt(etaPhi_sqrd_t), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_prop, nrow = 1), C2, log = TRUE))
  acc <- min(1, exp(log_acceptance))
  
  # Decision on the next step
  U = runif(1)
  if (s <- (acc > U))
    etaPhi_sqrd_next <- etaPhi_sqrd_prop
  else
    etaPhi_sqrd_next <- etaPhi_sqrd_t
  
  # Acceptance rate
  if (i==1) ac_etaPhi_sqrd[i] <- s
  else ac_etaPhi_sqrd[i] <- ac_etaPhi_sqrd[i-1]+s
  ac_rate_etaPhi_sqrd[i] <- ac_etaPhi_sqrd[i]/i
  
  
  if ((i%%cur_adapt==0) && ((median(ac_rate_etaPhi_sqrd[(i-cur_adapt/2):i])>.4) || (median(ac_rate_etaPhi_sqrd[(i-cur_adapt/2):i])<.3))) {
    adapt_eps_etaPhi_sqrd <- TRUE
    nb_adapt <- c(nb_adapt, i)
    ac_rate_window <- numeric(ac_rate_span)
  }
  
  if (adapt_eps_etaPhi_sqrd == TRUE){
    etaPhi_sqrd_adapt[1,] <- etaPhi_sqrd_next
    for (j in 1:(T_adapt-1)){
      etaPhi_sqrd_t <- etaPhi_sqrd_adapt[j,]
      etaPhi_sqrd_prop <-
        rdirichlet(1,
                   c(
                     etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
                     etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
                     etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1
                     ))
      # Computing acceptance probability
      C1 <-
        c(
          etaPhi_sqrd_prop[1] * eps_etaPhi_sqrd + 1,
          etaPhi_sqrd_prop[2] * eps_etaPhi_sqrd + 1,
          etaPhi_sqrd_prop[3] * eps_etaPhi_sqrd + 1
        )
      C2 <-
        c(etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
          etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
          etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1)
      log_acceptance <-
        (
          Log_post_etaPhi(p, sqrt(etaPhi_sqrd_prop), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_t, nrow = 1), C1 , log = TRUE)
        ) - (Log_post_etaPhi(p, sqrt(etaPhi_sqrd_t), sigma, mu, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_prop, nrow = 1), C2, log = TRUE))
      acc <- min(1, exp(log_acceptance))
      # Decision on the next step
      U = runif(1)
      if (s <- (acc>U)) etaPhi_sqrd_adapt[j+1,] <- etaPhi_sqrd_prop
      else etaPhi_sqrd_adapt[j+1,] <- etaPhi_sqrd_t
      
      ac_rate_window[j %% ac_rate_span + 1] <- s
      
      if (j >= ac_rate_span){
        current_ac_rate <- sum(ac_rate_window) /ac_rate_span
      
        # Adjust the eps_p 
        if (current_ac_rate < ac_rate_lower) eps_etaPhi_sqrd <- eps_etaPhi_sqrd*1.1
        else if (current_ac_rate > ac_rate_upper) eps_etaPhi_sqrd <- eps_etaPhi_sqrd*0.9
        }
    }
    adapt_eps_etaPhi_sqrd <- FALSE
  }
  
  
  etaPhi_sqrd[i+1,] <- etaPhi_sqrd_next
  
  # Update sigma1/2
  etaPhi <- sqrt(etaPhi_sqrd[i+1,])
  sigma1[i+1] <- sigma * (etaPhi[1] / sqrt(p))
  sigma2[i+1] <- sigma * (etaPhi[2] / sqrt(1 - p))
  mu1[i + 1] <- mu - sigma * etaPhi[3] * (sqrt(1 - p) / sqrt(p))
  mu2[i + 1] <- mu + sigma * etaPhi[3] * (sqrt(p) / sqrt(1 - p))
}

n_trunc <- 5000
mu1 <- mu1[n_trunc:length(mu1)]
mu2 <- mu2[n_trunc:length(mu2)]
sigma1 <- sigma1[n_trunc:length(sigma1)]
sigma2 <- sigma2[n_trunc:length(sigma2)]
```

```{r}
ggplot() +
  geom_line(aes(y=ac_rate_etaPhi_sqrd, x=seq(1,length(ac_rate_etaPhi_sqrd))))+
  geom_hline(yintercept = ac_rate_lower, linetype ="dashed", color  = "blue")+
  geom_hline(yintercept = ac_rate_upper, linetype ="dashed", color  = "blue")+
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin= ac_rate_lower, ymax=ac_rate_upper), alpha = .1, fill = "blue")+
  ggtitle("Evolution of the acceptance rate")+
  labs(y="Acceptance rate", x="Iteration number")
  
```

```{r}
ggplot() +
  geom_histogram(aes(x=mu1), bins = 100) +
  geom_vline(xintercept = mu_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu1), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(mu1)-mu_1)))

ggplot() +
  geom_histogram(aes(x=mu2), bins = 100) +
  geom_vline(xintercept = mu_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu2), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(mu2)-mu_2)))
```

```{r}
ggplot() +
  geom_histogram(aes(x=sigma1), bins = 100) +
  geom_vline(xintercept = sigma_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma1), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(sigma1)-sigma_1)))

ggplot() +
  geom_histogram(aes(x=sigma2), bins = 100) +
  geom_vline(xintercept = sigma_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma2), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(sigma2)-sigma_2)))
```

```{r}
source("plot_dirichlet.R")
plot_dirichlet(etaPhi_sqrd[n_trunc:nrow(etaPhi_sqrd),])
```

## Sampling all the parameters:

```{r}
Tau <- list("tau1"=10 , "tau2"=10)
Alpha <- c(.5,.5,.5)
ksi <- list("shape"=1.5, "rate"=2)
psi <- list("mean"=1, "sd"=1)
```

```{r}
source("Log_lik.R")
```

#### Posterior

```{r}
Log_post_p <- function (p_val, etaPhi_val, sigma_val, mu_val, Tau, dt) {
  return( Log_lik(p_val, etaPhi_val, sigma_val, mu_val, dt) + log(dbeta(p_val, Tau$tau1, Tau$tau2)))
}

Log_post_etaPhi <- function (p_val, etaPhi_val, sigma_val, mu_val, Alpha, dt){
  return( Log_lik(p_val, etaPhi_val, sigma_val, mu_val, dt) + ddirichlet(matrix(etaPhi_val, nrow = 1), Alpha, log = TRUE))
}

Log_post_sigma <- function ( p_val, etaPhi_val, sigma_val, mu_val, ksi, dt){
  return( Log_lik(p_val, etaPhi_val, sigma_val, mu_val, dt) + dinvgamma(sigma_val, ksi$shape, ksi$rate, log = TRUE))
}

Log_post_mu <- function (p_val, etaPhi_val, sigma_val, mu_val, psi, dt){
  return( Log_lik(p_val, etaPhi_val, sigma_val, mu_val, dt) + dnorm(mu_val,psi$mean, psi$sd, log = TRUE ))
}
```

#### Sampling with adaptation

```{r}
# Nbr iterations
T <- 50000
T_adapt <- 300

# Mixture weights p
eps_p <- 0.4
P <- numeric(length = T)
p_0 <- rbeta(1,.5, .5)
P[1] <- p_0
P_adapt <- numeric(length = T_adapt)

# Mixture mean 
eps_mu <- .4
Mu <- numeric(length = T)
Mu[1] <- mean(Y)
Mu_adapt <- numeric(length = T_adapt)

# Mixture std sigma
eps_sigma_sqrd <- .4
sigma_sqrd <- numeric(length = T)
sigma_sqrd[1] <- sd(Y)
sigma_sqrd_adapt <- numeric(length = T_adapt)

# Component ecpectations mu1/2
mu1_0 <- 1
mu2_0 <- 1
mu1 <- vector(mode = "numeric", length = T)
mu2 <- vector(mode = "numeric", length = T)
mu1[1] <- mu1_0
mu2[1] <- mu2_0

# Component std sigma1/2
sigma1_0 <- 1
sigma2_0 <- 1
sigma1 <- vector(mode = "numeric", length = T)
sigma2 <- vector(mode = "numeric", length = T)
sigma1[1] <- sigma1_0
sigma2[1] <- sigma2_0

# Initialization etaPhi (contient les carres de eta_1, eta_2 et Phi )
eps_etaPhi_sqrd <- 300
etaPhi_sqrd_0 <- rdirichlet(1, Alpha)
etaPhi_sqrd <- matrix(NA, T, 3)
etaPhi_sqrd[1, ] <- etaPhi_sqrd_0
etaPhi_sqrd_adapt <- matrix(NA, T_adapt, 3)

# Algorithm vitals
ac_p <-  logical(length = T-1)
ac_rate_p <- numeric(length = T-1)

ac_sigma_sqrd <- logical(length = T-1)
ac_rate_sigma_sqrd <- numeric(length = T-1)

ac_etaPhi_sqrd <-  logical(length = T-1)
ac_rate_etaPhi_sqrd <- numeric(length = T-1)

ac_mu <- logical(length = T-1)
ac_rate_mu <- numeric(length = T-1)

# Adaptation
ac_rate_lower <- .325
ac_rate_upper <- .375
adapt_eps_etaPhi_sqrd <- FALSE
adapt_eps_p <- FALSE
adapt_eps_mu <- FALSE
adapt_eps_sigma_sqrd <- FALSE
cur_adapt <- 300
ac_rate_span <- 75
nb_adapt <- NULL

for (i in 1:(T - 1)) {
  # Initialize the Gibbs iteration
  p_t <- P[i]
  etaPhi_sqrd_t <- etaPhi_sqrd[i,]
  sigma_sqrd_t <- sigma_sqrd[i]
  mu_t <- Mu[i]
  ##=======================Sample the mixture weight=====================================>
    #Generate a proposal
    p_prop <- rbeta(1, (p_t*eps_p)+1, ((1-p_t)*eps_p)+1)

    # Computing acceptance
    log_acceptance <- 
      (Log_post_p(p_prop, sqrt(etaPhi_sqrd_t), sqrt(sigma_sqrd_t), mu_t, Tau, Y) + dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1, log = TRUE)) -
      (Log_post_p(p_t, sqrt(etaPhi_sqrd_t), sqrt(sigma_sqrd_t), mu_t, Tau, Y) + dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1, log = TRUE))
    acc_p <- min(1, exp(log_acceptance))
    
    # Decision on the next step 
    U = runif(1)
    if (s <- (acc_p>U)) p_next <- p_prop
    else p_next <- p_t
    # Acceptance rate
    if (i==1) ac_p[i] <- s
    else ac_p[i] <- ac_p[i-1]+s
    ac_rate_p[i] <- ac_p[i]/i
    
    
    if ((i%%cur_adapt==0) && ((median(ac_rate_p[(i-cur_adapt/2):i])>.4) || (median(ac_rate_p[(i-cur_adapt/2):i])<.3))) {
      i_start <- i
      adapt_eps_p <- TRUE
      ac_rate_window <- numeric(ac_rate_span)
    }
    
    if (adapt_eps_p == TRUE){
      P_adapt[1] <- p_next
      for (j in 1:T_adapt){
        p_t <- P_adapt[j]
        p_prop <- rbeta(1, p_t*eps_p+1, (1-p_t)*eps_p+1)
        
        # Computing acceptance probability
        log_acceptance <- 
        (Log_post_p(p_prop, sqrt(etaPhi_sqrd_t), sqrt(sigma_sqrd_t), mu_t, Tau, Y) + dbeta(p_t, p_prop*eps_p+1, (1-p_prop)*eps_p+1, log = TRUE)) -
        (Log_post_p(p_t, sqrt(etaPhi_sqrd_t), sqrt(sigma_sqrd_t), mu_t, Tau, Y) + dbeta(p_prop, p_t*eps_p+1, (1-p_t)*eps_p+1, log = TRUE))
        acc_p <- min(1, exp(log_acceptance))
          
        # Decision on the next step
        U = runif(1)
        if (s <- (acc_p>U)) P_adapt[j+1] <- p_prop
        else P_adapt[j+1] <- p_t
        
        ac_rate_window[j %% ac_rate_span + 1] <- s
        
        if (j >= ac_rate_span){
          current_ac_rate <- sum(ac_rate_window) / ac_rate_span
        
          # Adjust the eps_p 
          if (current_ac_rate < ac_rate_lower) eps_p <- eps_p*1.1
          else if (current_ac_rate > ac_rate_upper) eps_p <- eps_p*0.9
        }
      }
      adapt_eps_p <- FALSE
    }    
    
    
    P[i+1] <- p_next
  ##======================Sample the eta1, et2 and phi all squared========================>
    # Generate a proposal
    etaPhi_sqrd_prop <-
    rdirichlet(1,
               c(
                 etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
                 etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
                 etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1
               ))

    # Computing acceptance probability
    tmp1 <-
      c(
        etaPhi_sqrd_prop[1] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_prop[2] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_prop[3] * eps_etaPhi_sqrd + 1
      )
    tmp2 <-
      c(etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
        etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1)
    
    log_acceptance <-
      (
        Log_post_etaPhi(p_next, sqrt(etaPhi_sqrd_prop), sqrt(sigma_sqrd_t), mu_t, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_t, nrow = 1), tmp1 , log = TRUE)
      ) - (Log_post_etaPhi(p_next, sqrt(etaPhi_sqrd_t), sqrt(sigma_sqrd_t), mu_t, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_prop, nrow = 1), tmp2, log = TRUE))
    acc_etaPhi_sqrd <- min(1, exp(log_acceptance))

    # Decision on the next step
    U = runif(1)
    if (s <- (acc_etaPhi_sqrd > U))
      etaPhi_sqrd_next <- etaPhi_sqrd_prop
    else
      etaPhi_sqrd_next <- etaPhi_sqrd_t
    
    # Acceptance rate
    if (i==1) ac_etaPhi_sqrd[i] <- s
    else ac_etaPhi_sqrd[i] <- ac_etaPhi_sqrd[i-1]+s
    ac_rate_etaPhi_sqrd[i] <- ac_etaPhi_sqrd[i]/i
    
    
    if ((i%%cur_adapt==0) && ((mean(ac_rate_etaPhi_sqrd[(i-cur_adapt/2):i])>.4) || (mean(ac_rate_etaPhi_sqrd[(i-cur_adapt/2):i])<.3))) {
      adapt_eps_etaPhi_sqrd <- TRUE
      nb_adapt <- c(nb_adapt, i)
      ac_rate_window <- numeric(ac_rate_span)
    }
    
    if (adapt_eps_etaPhi_sqrd == TRUE){
      etaPhi_sqrd_adapt[1,] <- etaPhi_sqrd_next
      for (j in 1:(T_adapt-1)){
        etaPhi_sqrd_t <- etaPhi_sqrd_adapt[j,]
        etaPhi_sqrd_prop <-
          rdirichlet(1,
                     c(
                       etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
                       etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
                       etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1
                       ))
        # Computing acceptance probability
        C1 <-
          c(
            etaPhi_sqrd_prop[1] * eps_etaPhi_sqrd + 1,
            etaPhi_sqrd_prop[2] * eps_etaPhi_sqrd + 1,
            etaPhi_sqrd_prop[3] * eps_etaPhi_sqrd + 1
          )
        C2 <-
          c(etaPhi_sqrd_t[1] * eps_etaPhi_sqrd + 1,
            etaPhi_sqrd_t[2] * eps_etaPhi_sqrd + 1,
            etaPhi_sqrd_t[3] * eps_etaPhi_sqrd + 1)
        log_acceptance <-
          (
            Log_post_etaPhi(p_next, sqrt(etaPhi_sqrd_prop), sqrt(sigma_sqrd_t), mu_t, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_t, nrow = 1), C1 , log = TRUE)
          ) - (Log_post_etaPhi(p_next, sqrt(etaPhi_sqrd_t), sqrt(sigma_sqrd_t), mu_t, Alpha, Y) + ddirichlet(matrix(etaPhi_sqrd_prop, nrow = 1), C2, log = TRUE))
        acc <- min(1, exp(log_acceptance))
        # Decision on the next step
        U = runif(1)
        if (s <- (acc>U)) etaPhi_sqrd_adapt[j+1,] <- etaPhi_sqrd_prop
        else etaPhi_sqrd_adapt[j+1,] <- etaPhi_sqrd_t
        
        ac_rate_window[j %% ac_rate_span + 1] <- s
        
        if (j >= ac_rate_span){
          current_ac_rate <- sum(ac_rate_window) / ac_rate_span
        
          # Adjust the eps_p 
          if (current_ac_rate < ac_rate_lower) eps_etaPhi_sqrd <- eps_etaPhi_sqrd*1.1
          else if (current_ac_rate > ac_rate_upper) eps_etaPhi_sqrd <- max(0, eps_etaPhi_sqrd*0.9)
          }
      }
      adapt_eps_etaPhi_sqrd <- FALSE
    }
    
    
    etaPhi_sqrd[i+1,] <- etaPhi_sqrd_next
    # Update sigma1/2
    etaPhi <- sqrt(etaPhi_sqrd[i+1,])
    sigma <- sqrt(sigma_sqrd_t)
    mu <- mu_t
    sigma1[i+1] <- sigma * (etaPhi[1] / sqrt(p_next))
    sigma2[i+1] <- sigma * (etaPhi[2] / sqrt(1 - p_next))
    mu1[i + 1] <- mu - sigma * etaPhi[3] * (sqrt(1 - p_next) / sqrt(p_next))
    mu2[i + 1] <- mu + sigma * etaPhi[3] * (sqrt(p_next) / sqrt(1 - p_next))
    

    ##======================Sample standard deviation========================>
    sigma_sqrd_prop <- rinvgamma(1, 2+ ((sigma_sqrd_t^2)/eps_sigma_sqrd), sigma_sqrd_t * ( 1 + (sigma_sqrd_t^2)/eps_sigma_sqrd ))
    
    
    log_acceptance <-
      (
        Log_post_sigma(p_next,sqrt(etaPhi_sqrd_next), sqrt(sigma_sqrd_prop), mu_t, ksi, Y) + dinvgamma(sigma_sqrd_t, 2+ ((sigma_sqrd_t^2)/eps_sigma_sqrd), sigma_sqrd_t * ( 1 + (sigma_sqrd_t^2)/eps_sigma_sqrd ), log = TRUE)
      ) - (Log_post_sigma(p_next, sqrt(etaPhi_sqrd_next), sqrt(sigma_sqrd_t), mu_t, ksi, Y) + dinvgamma(sigma_sqrd_prop,2+ ((sigma_sqrd_prop^2)/eps_sigma_sqrd), sigma_sqrd_prop * ( 1 + (sigma_sqrd_prop^2)/eps_sigma_sqrd ),  log = TRUE))
    acc_sigma_sqrd <- min(1, exp(log_acceptance))
    
    
    # Decision on the next step
    U = runif(1)
    if (s <- (acc_sigma_sqrd > U))
      sigma_sqrd_next <- sigma_sqrd_prop
    else
      sigma_sqrd_next <- sigma_sqrd_t
    
    # Acceptance rate
    if (i==1) ac_sigma_sqrd[i] <- s
    else ac_sigma_sqrd[i] <- ac_sigma_sqrd[i-1]+s
    ac_rate_sigma_sqrd[i] <- ac_sigma_sqrd[i]/i
    
    sigma_sqrt <- sqrt(sigma_sqrd_next)
    # Update 
    sigma1[i+1] <- sigma_sqrt * (etaPhi[1] / sqrt(p_next))
    sigma2[i+1] <- sigma_sqrt * (etaPhi[2] / sqrt(1 - p_next))
    mu1[i + 1] <- mu - sigma_sqrt * etaPhi[3] * (sqrt(1 - p_next) / sqrt(p_next))
    mu2[i + 1] <- mu + sigma_sqrt * etaPhi[3] * (sqrt(p_next) / sqrt(1 - p_next))
    
    
    if ((i%%cur_adapt==0) && ((median(ac_rate_sigma_sqrd[(i-cur_adapt/2):i])>.4) || (median(ac_rate_sigma_sqrd[(i-cur_adapt/2):i])<.3))) {
      nb_adapt <- c(nb_adapt, i)
      adapt_eps_sigma_sqrd <- TRUE
      ac_rate_window <- numeric(ac_rate_span)
    }
    
    if (adapt_eps_sigma_sqrd == TRUE){
      sigma_sqrd_adapt[1] <- sigma_sqrd_next
      for (j in 1:T_adapt){
        sigma_sqrd_t <- sigma_sqrd_adapt[j]
        sigma_sqrd_prop <- rinvgamma(1, 2+ ((sigma_sqrd_t^2)/eps_sigma_sqrd), sigma_sqrd_t * ( 1 + (sigma_sqrd_t^2)/eps_sigma_sqrd ))
        print
        # Computing acceptance probability
        log_acceptance <-
        (
          Log_post_sigma(p_next,sqrt(etaPhi_sqrd_next), sqrt(sigma_sqrd_prop), mu_t, ksi, Y) + dinvgamma(sigma_sqrd_t, 2+ ((sigma_sqrd_t^2)/eps_sigma_sqrd), sigma_sqrd_t * ( 1 + (sigma_sqrd_t^2)/eps_sigma_sqrd ), log = TRUE)
        ) - (Log_post_sigma(p_next, sqrt(etaPhi_sqrd_next), sqrt(sigma_sqrd_t), mu_t, ksi, Y) + dinvgamma(sigma_sqrd_prop,2+ ((sigma_sqrd_prop^2)/eps_sigma_sqrd), sigma_sqrd_prop * ( 1 + (sigma_sqrd_prop^2)/eps_sigma_sqrd ),  log = TRUE))
      acc_sigma_sqrd <- min(1, exp(log_acceptance))
      
        # Decision on the next step
        U = runif(1)
        if (s <- (acc_sigma_sqrd > U))
          sigma_sqrd_adapt[j+1] <- sigma_sqrd_prop
        else
          sigma_sqrd_adapt[j+1] <- sigma_sqrd_t
        
        ac_rate_window[j %% ac_rate_span + 1] <- s
        
        if (j >= ac_rate_span){
          current_ac_rate <- sum(ac_rate_window) / ac_rate_span
          
          # Adjust the eps_p
          if (current_ac_rate < ac_rate_lower) eps_sigma_sqrd <- eps_sigma_sqrd*0.95
          else if (current_ac_rate > ac_rate_upper) eps_sigma_sqrd <- eps_sigma_sqrd*1.05
        }
      }
      adapt_eps_sigma_sqrd <- FALSE
      
      
    }
    sigma <- sqrt(sigma_sqrd_next)
    etaPhi <- sqrt(etaPhi_sqrd_next)
    sigma1[i+1] <- sigma * (etaPhi[1] / sqrt(p_next))
    sigma2[i+1] <- sigma * (etaPhi[2] / sqrt(1 - p_next))
    mu1[i + 1] <- mu - sigma * etaPhi[3] * (sqrt(1 - p_next) / sqrt(p_next))
    mu2[i + 1] <- mu + sigma * etaPhi[3] * (sqrt(p_next) / sqrt(1 - p_next))
    
    sigma_sqrd[i+1] <- sigma_sqrd_next

    ##======================Sample mean========================>
    mu_prop <- rnorm(1, mean = mu_t, eps_mu)
    
    
    log_acceptance <-
      (
        Log_post_mu(p_next,sqrt(etaPhi_sqrd_next), sqrt(sigma_sqrd_next), mu_prop, psi, Y) + dnorm(mu_t,mu_prop, eps_mu, log = TRUE)
      ) - (Log_post_mu(p_next,sqrt(etaPhi_sqrd_next), sqrt(sigma_sqrd_next), mu_t, psi, Y) + dnorm(mu_prop,mu_t, eps_mu, log = TRUE))
    acc_mu <- min(1, exp(log_acceptance))
    

    # Decision on the next step
    U = runif(1)
    if (s <- (acc_mu > U))
      mu_next <- mu_prop
    else
      mu_next <- mu_t
    
    # Acceptance rate
    if (i==1) ac_mu[i] <- s
    else ac_mu[i] <- ac_mu[i-1]+s
    ac_rate_mu[i] <- ac_mu[i]/i
    
    mu <- mu_next
    # Update 
    sigma_sqrt <- sqrt(sigma_sqrd_next)
    sigma1[i+1] <- sigma_sqrt * (etaPhi[1] / sqrt(p_next))
    sigma2[i+1] <- sigma_sqrt * (etaPhi[2] / sqrt(1 - p_next))
    mu1[i+1] <- mu - sigma_sqrt * etaPhi[3] * (sqrt(1 - p_next) / sqrt(p_next))
    mu2[i+1] <- mu + sigma_sqrt * etaPhi[3] * (sqrt(p_next) / sqrt(1 - p_next))
    
    
    if ((i%%cur_adapt==0) && ((median(ac_rate_mu[(i-cur_adapt/2):i])>.4) || (median(ac_rate_mu[(i-cur_adapt/2):i])<.3))) {
      nb_adapt <- c(nb_adapt, i)
      adapt_eps_mu <- TRUE
      ac_rate_window <- numeric(ac_rate_span)
    }
  
    if (adapt_eps_mu == TRUE){
      Mu_adapt[1] <- mu_next
      for (j in 1:T_adapt){
        mu_t <- Mu_adapt[j]
        mu_prop <- rnorm(1, mean = mu_t, eps_mu)
  
        # Computing acceptance probability
        log_acceptance <-
      (
        Log_post_mu(p_next,sqrt(etaPhi_sqrd_next), sqrt(sigma_sqrd_next), mu_prop, psi, Y) + dnorm(mu_t,mu_prop, eps_mu, log = TRUE)
      ) - (Log_post_mu(p_next,sqrt(etaPhi_sqrd_next), sqrt(sigma_sqrd_next), mu_t, psi, Y) + dnorm(mu_prop,mu_t, eps_mu, log = TRUE))
    acc_mu <- min(1, exp(log_acceptance))
        # Decision on the next step
        U = runif(1)
        if (s <- (acc_mu > U))
          Mu_adapt[j+1] <- mu_prop
        else
          Mu_adapt[j+1] <- mu_t
  
        ac_rate_window[j %% ac_rate_span + 1] <- s
  
        if (j >= ac_rate_span){
          current_ac_rate <- sum(ac_rate_window) / ac_rate_span
  
          # Adjust the eps_p
          if (current_ac_rate < ac_rate_lower) eps_mu <- eps_mu*0.95
          else if (current_ac_rate > ac_rate_upper) eps_mu <- eps_mu*1.05
        }
      }
      adapt_eps_mu <- FALSE
  
  
    }
    mu <- mu_next
    sigma <- sqrt(sigma_sqrd_next)
    etaPhi <- sqrt(etaPhi_sqrd_next)
    sigma1[i+1] <- sigma * (etaPhi[1] / sqrt(p_next))
    sigma2[i+1] <- sigma * (etaPhi[2] / sqrt(1 - p_next))
    mu1[i + 1] <- mu - sigma * etaPhi[3] * (sqrt(1 - p_next) / sqrt(p_next))
    mu2[i + 1] <- mu + sigma * etaPhi[3] * (sqrt(p_next) / sqrt(1 - p_next))
    
    Mu[i+1] <- mu_next
}

n_trunc <- T/10
sigma_sqrd <- sigma_sqrd[n_trunc:length(sigma_sqrd)]
sigma_ <- sqrt(sigma_sqrd)
Mu <- Mu[n_trunc:length(Mu)]
P <- P[n_trunc:length(P)]
sigma1 <- sigma1[n_trunc:length(sigma1)]
sigma2 <- sigma2[n_trunc:length(sigma2)]
mu1 <- mu1[n_trunc:length(mu1)]
mu2 <- mu2[n_trunc:length(mu2)]



```

```{r}
ggplot() +
  geom_line(aes(y=ac_rate_etaPhi_sqrd, x=seq(1,length(ac_rate_etaPhi_sqrd))))+
  geom_hline(yintercept = ac_rate_lower, linetype ="dashed", color  = "blue")+
  geom_hline(yintercept = ac_rate_upper, linetype ="dashed", color  = "blue")+
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin= ac_rate_lower, ymax=ac_rate_upper), alpha = .1, fill = "blue")+
  ggtitle("Evolution of the acceptance rate of etaPhi")+
  labs(y="Acceptance rate", x="Iteration number")

ggplot() +
  geom_line(aes(y=ac_rate_p, x=seq(1,length(ac_rate_p))))+
  geom_hline(yintercept = ac_rate_lower, linetype ="dashed", color  = "blue")+
  geom_hline(yintercept = ac_rate_upper, linetype ="dashed", color  = "blue")+
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin= ac_rate_lower, ymax=ac_rate_upper), alpha = .1, fill = "blue")+
  ggtitle("Evolution of the acceptance rate of p")+
  labs(y="Acceptance rate", x="Iteration number")

ggplot() +
  geom_line(aes(y=ac_rate_sigma_sqrd, x=seq(1,length(ac_rate_sigma_sqrd))))+
  geom_hline(yintercept = ac_rate_lower, linetype ="dashed", color  = "blue")+
  geom_hline(yintercept = ac_rate_upper, linetype ="dashed", color  = "blue")+
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin= ac_rate_lower, ymax=ac_rate_upper), alpha = .1, fill = "blue")+
  ggtitle("Evolution of the acceptance rate of sigma")+
  labs(y="Acceptance rate", x="Iteration number")

ggplot() +
  geom_line(aes(y=ac_rate_mu, x=seq(1,length(ac_rate_mu))))+
  geom_hline(yintercept = ac_rate_lower, linetype ="dashed", color  = "blue")+
  geom_hline(yintercept = ac_rate_upper, linetype ="dashed", color  = "blue")+
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin= ac_rate_lower, ymax=ac_rate_upper), alpha = .1, fill = "blue")+
  ggtitle("Evolution of the acceptance rate of mu")+
  labs(y="Acceptance rate", x="Iteration number")

```

```{r}
ggplot() +
  geom_histogram(aes(x=P), bins = 200) +
  geom_vline(xintercept = p, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(P), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(P)-p)))
```

```{r}

ggplot() +
  geom_histogram(aes(x=sigma1), bins = 200) +
  geom_vline(xintercept = sigma_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma1), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(sigma1)-sigma_1)))


ggplot() +
  geom_histogram(aes(x=sigma2), bins = 200) +
  geom_vline(xintercept = sigma_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(sigma2), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(sigma2)-sigma_2)))
```

```{r}
ggplot() +
  geom_histogram(aes(x=mu1), bins = 100) +
  geom_vline(xintercept = mu_1, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu1), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(mu1)-mu_1)))


ggplot() +
  geom_histogram(aes(x=mu2), bins = 100) +
  geom_vline(xintercept = mu_2, color="red", show.legend = TRUE)  +
  geom_vline(xintercept = mean(mu2), color="blue", show.legend = TRUE) +
  ggtitle(sprintf("err=%.6f", abs(mean(mu2)-mu_2)))
```

```{r}
p_est <- mean(P) #Mixture weight

# Component parameters
sigma_1_est <- mean(sigma1)
sigma_2_est <- mean(sigma2)
mu_1_est <- mean(mu1)
mu_2_est <- mean(mu2)

Mix_est <- rnormix(N, p_est,mu_1_est, sigma_1_est,  mu_2_est, sigma_2_est)
Y_est <- Mix_est$samples
```

```{r}
ggplot() + 
  geom_histogram(aes(Y), bins = 100, alpha=.5, fill = "blue")+
  geom_histogram(aes(Y_est), bins = 100, alpha=.5, fill= "green")+
  xlab("")
```

```{r}
test <- ks.test(exp(Y), exp(Y_est))
print(test)
```

```{r}
qqplot(Y, Y_est)
abline(a = 0, b = 1, col = "red")
```
